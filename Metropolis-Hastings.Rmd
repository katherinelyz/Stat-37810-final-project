---
title: 'STAT 37810 Final Project #1: Metropolis-Hastings'
author: "Yingzhao Li & Linlin Wu"
date: "Nov. 1st, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##1 Algorithm and implementation

### The Beta Meteropolis-Hastings alogrithm

```{r algorithm}
betametropolis <- function(startvalue, c, iterations = 10000){
  chain = rep(0, iterations)
  for (i in 1:iterations){
    currentvalue <- rbeta(1, c * startvalue, c * (1 - startvalue))
    proposal <- dbeta(startvalue, c * currentvalue, c * (1 - currentvalue)) / dbeta(currentvalue, c * startvalue, c * (1 - startvalue))
    posterior <- dbeta(currentvalue, 6, 4) / dbeta(startvalue, 6, 4)
    if (runif(1) < min(1, posterior * proposal)){
      chain[i] <- currentvalue
      startvalue <- currentvalue
    }else{
      chain[i] <- startvalue
    }
  }
  return(chain)
}
```

The Beta Meteropolis-Hastings alogrithm is defined as a function which starts with number of iteration of 0's called the *chain*.

The *currentvalue* is a proposal function of the form $$\phi_{prop}|\phi_{old} \sim Beta(c\phi_{old}, c(1-\phi_{old}))$$
where the *startvalue* = $\phi_{old}$, with 1 observation.

The *proposal* is where we choose a new parameter value close to the old value based on the probability density function.

The *posterior* is the probability of $P(currentvalue)/P(startvalue)$, where P is the Beta distribution, with parameter (6,4).

Then, we will store the results into the *chain*. Since the Beta distribution is not symmetric, we store the *currentvalue* into *chain* and replace the *startvalue* with the *currentvalue* if both $posterior*proposal$ and 1 is greater than a random uniform deviates. Otherwise, the old *startvalue* is stored.

Finally, this function will return the value of the *chain*.

### Implementation

```{r implementation}
# generate a starting value from a uniform distribution
startvalue <- runif(1)
# set c=1 and run for 10,000 total iterations
draws <- betametropolis(startvalue, 1, 10000)
```

This implements the alogrithm with c = 1 and runs for 10,000 total iterations. The starting value is generated from a uniform distribution. Call the resulting sampler *draws*.

##2 Evaluate the performance of the sampler with c = 1

### Trace plot, autocorrelation plot, and histogram of the draws with c = 1

```{r plot1}
# plots for c = 1
par(mfrow = c(1, 3))
plot(draws, type = "l", main = "Trace plot: c = 1"); acf(draws, main = "Autocorrelation plot: c = 1"); hist(draws, main = "Histogram: c = 1")
```

### Compare the draws with the target distribution Beta(6,4)

```{r plot2}
# histogram of the draws with Beta(6,4)
par(mfrow = c(1, 1))
hist(draws, freq = FALSE, main = "Histogram: c = 1")
x<-seq(0, 1, 0.01)
lines(x, dbeta(x, 6, 4))
```

We can see that graphically, the target distribution Beta(6,4) is not a very good fit for the histogram of the draws. 

```{r ks.test}
#ks test
ks.test(jitter(draws, 0.0001), rbeta(10000, 6, 4))
```

Most of the time, the p-value of the Kolmogorovâ€“Smirnov test is less than 0.05. So, it's very likely that we will reject the null hypothesis that the draws and the target distribution Beta(6,4) are the same.

Therefore, in general, the performance of the sampler with c = 1 is not quite good.

##3 Re-run this sampler with c = 0.1, c = 2.5 and c = 10

### Trace plot, autocorrelation plot, and histograms with c = 0.1, c = 2.5 and c = 10

```{r plot3}
# plots for c = 0.1, 2.5, 10
cvec<-c(0.1, 2.5, 10)
for (i in 1:length(cvec)){
  par(mfrow=c(1, 3))
  draws=betametropolis(startvalue, cvec[i], 10000)
  plot(draws, type = "l", main = paste("Trace plot: c = ", cvec[i])); acf(draws, main = paste("Autocorrelation plot: c = ", cvec[i])); hist(draws, freq = FALSE, main = paste ("Histogram: c = ", cvec[i]))
  x<-seq(0, 1, 0.01)
  lines(x, dbeta(x, 6, 4))
}
```

From the autocorrelation plots, we can see that c = 1 has most lags, which requires thinning. c = 2.5 has least lags, which is the most effective at drawing from the target distribution.

From the histograms, we can see that c = 10 seems to have the best fit for the target distribution Beta(6,4).

### The number of draws needed from the sampler before thinning and burn-in, and the comparison to the properties of the target distribution.

Set burnin = 10% of the total iterations as default.

```{r thinning&burn-in}
betametropolis <- function(startvalue, c, thin, burnin=100, iterations=10000){
  chain = rep(0, iterations)
  for (i in 1:iterations){
    for (j in 1:thin){
      currentvalue <- rbeta(1, c * startvalue, c * (1 - startvalue))
      proposal <- dbeta(startvalue, c * currentvalue, c * (1 - currentvalue)) / dbeta(currentvalue, c * startvalue, c * (1 - startvalue))
      posterior <- dbeta(currentvalue, 6, 4) / dbeta(startvalue, 6, 4)
      if (runif(1) < min(1, posterior * proposal)){
        chain[i] <- currentvalue
        startvalue <- currentvalue
      }else{
        chain[i] <- startvalue
      }
    }
  }
  return(chain[-(1:burnin)])
}

startvalue <- runif(1)
```

```{r}
# set thin = 50
draws1 <- betametropolis(startvalue, 0.1, 50, 1000, 10000)
ks.test(jitter(draws1, 0.0001), rbeta(10000, 6, 4))
```

For c = 0.1, we need to thin about 50 times to make it very effective at drawing from the target distribution. So, in general, 500000 draws are needed from the sampler before thinning and burn-in. Thus, for c = 0.1, it's not very effective at drawing from the target distribution with a total of 10000 total iterations and without thinning and burn-in.

```{r}
# set thin = 5
draws2 <- betametropolis(startvalue, 2.5, 5, 1000, 10000)
ks.test(jitter(draws2, 0.0001), rbeta(10000, 6, 4))
```

For c = 2.5, we need to thin about 5 times to make it very effective at drawing from the target distribution. So, in general, 50000 draws are needed from the sampler before thinning and burn-in. Thus, for c = 2.5, it's relative effective at drawing from the target distribution with a total of 10000 total iterations and without thinning and burn-in.


```{r}
# set thin = 5
draws3 <- betametropolis(startvalue, 10, 5, 1000, 10000)
ks.test(jitter(draws3, 0.0001), rbeta(10000, 6, 4))
```

For c = 10, we need to thin about 5 times to make it very effective at drawing from the target distribution. So, in general, 50000 draws are needed from the sampler before thinning and burn-in. Thus, for c = 10, it's relative effective at drawing from the target distribution with a total of 10000 total iterations and without thinning and burn-in.

Therefore, after comparing the number of draws needed from the sampler before thinning and burn-in for c = 0.1, c = 2.5 and c = 10, we can conclude that c = 2.5 and c = 10 are more effective at drawing from the target distribution and better fit for Beta(6,4).
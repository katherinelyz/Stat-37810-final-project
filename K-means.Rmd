---
title: "STAT 37810 Final Project #3: K-Means"
author: "Yingzhao Li & Linlin Wu"
date: "Nov. 1st, 2016"
output: html_document
---

## Install packages
Package "rattle" is required for calling the wine data.

Package "fpc" is required for ploting the K-Means.

Package "fields" is required for calculating the Euclidean distance. 

Package "flexclust" is required for quantifying the algorithm.

```{r packages,eval=FALSE}
install.packages("rattle")
install.packages("fpc")
install.packages("fields")
install.packages("flexclust")
```

### Note
Please install all required package before running the following code.

```{r library,include=FALSE}
library(rattle)
library(fpc)
library(fields)
library(flexclust)
```

## K-Means function
The K-means function is defined as below. Euclidean distance measure is used in the K-means function.

```{r K-Means algorithm}
# The K-Means algorithm:
my_kmeans <- function(data, n) {
  # Change the dataset to matrix form
  kdata <- as.matrix(data)
  
  # Randomly select n cluster centers as a startcenter
  startcenter <- kdata[sample(nrow(kdata), n),]
  currentcenter <- startcenter[]*0 
  currentcluster <- rep(0, nrow(kdata))
  
  # Repeat the following process until no data point is reassigned
  repeat {
      # Calculate Euclidean distance between each data point and cluster centers
      centerdist <- rdist(kdata, startcenter)
      # Assign each data point to the nearest cluster center
      for (j in 1:nrow(kdata)){
      currentcluster[j] <- which.min(centerdist[j,])
      }
      # Derive the new cluster centers
      for (k in 1:n) {
      currentcenter[k,]<- colMeans(kdata[currentcluster == k,])
      }
      # Stop the process if the new cluster centers are the same as the old ones
      if (identical(startcenter, currentcenter) == TRUE){
        break
      }
      # Otherwise, saving the new cluster centers
      else{
        startcenter <- currentcenter
      }
    }
    
  # Return the resulting K-Means clusters and centers
  list(clusters = currentcluster, centers = currentcenter)
}
```

## Implement K-Means

### Wine data

```{r wine}
# Call wine dataset and exclude the "Type" variable
data.wine <- (wine[-1]) 

# Implement K-Means
fit.wine <- my_kmeans(data.wine, 3)
plotcluster(data.wine, fit.wine$clusters)
```

As we can see from the plot, the clusters are not very well seperated. There are some overlaps between each cluster.

To quantify how well the algorithm's clusters correspond to the three wine types, we will use the randIndex.

```{r wine randIndex}
ct.wine <- table(wine$Type, fit.wine$clusters)
randIndex(ct.wine)
```

The confuse table is used to compare the K-Means clusters and the original classes of the dataset, which is the types of wines. 

The randIndex gives a measure of the agreement between two partitions, adjusted for chance. It ranges from -1 (no agreement) to 1 (perfect agreement).

In our case, the randIndex is below 0.4, which indicates that the agreement between the wine type and the cluster solution is not very strong.

### Scaled wine data

```{r scaled wine}
data2.wine <- scale(data.wine) 
```

This command scales the orgininal data. We need to scale the data to perform accuracy of distance of each clusters. The K-means minimizes the error function, so normalizing the data improves convergence of such algorithms.

```{r K-Means for scaled wine}
fit2.wine <- my_kmeans(data2.wine, 3)
plotcluster(data2.wine, fit2.wine$clusters)
```

Repeat the steps using scaled wine data. 

Now, the data is clustered very well, with no overlaps between clusters.

```{r scaled wine randIndex}
ct2.wine <- table(wine$Type, fit2.wine$clusters)
randIndex(ct2.wine)
```

After the data is scaled, the randIndex becomes very strong, which is around 0.9. The scaled clusters correspond to the three wine types very well.

### Iris data

```{r iris}
# Call iris dataset and remove the "species" variable
data.iris <- iris[-5] 

# Implement K-Means
fit.iris <- my_kmeans(data.iris, 3)
plotcluster(data.iris, fit.iris$clusters)
```

From the plot, we can see that the 3 clusters are not well seperated.

```{r iris randIndex}
ct.iris <- table(iris$Species,fit.iris$clusters)
randIndex(ct.iris)
```

The randIndex is either around 0.4 or 0.7, which indicates that the agreement between the iris species and the cluster solution is not very strong.

### Scaled iris data

```{r scaled iris}
data2.iris <- scale(data.iris)

fit2.iris <- my_kmeans(data2.iris, 3)
plotcluster(data2.iris, fit2.iris$clusters)
```

After the iris data is scaled, the clusters still have overlaps.

```{r scaled iris randIndex}
ct2.iris <- table(iris$Species,fit2.iris$clusters)
randIndex(ct2.iris)
```

The randIndex of scaled iris is either around 0.4 or 0.6, which is close to that of the original data. Thus, the agreement between iris species and the scaled clusters is still not strong.

Therefore, K-Means does not work well for classifying the iris data and scaling does not help improving classifying it.
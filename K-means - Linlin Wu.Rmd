---
title: "K-Means"
author: "Linlin Wu"
date: "2016-11-5"
output: html_document
---

##Install packages
###Note
Package "rattle" is required for calling the wine data.

Package "fpc" is required fro ploting the k means.

Package "fields" is required for calculating the Euclidean distance. 

Package "flexclust" is required for quantify the algorithm.

```{r packages,eval=FALSE}
install.packages("rattle")
install.packages("fpc")
install.packages("fields")
install.packages("flexclust")
```

Please install all required package before running the following code.

```{r library,include=FALSE}
library(rattle)
library(fpc)
library(fields)
library(flexclust)
```

##K-means function
The K means function is defined as below.
Euclidean distance measure method is used in the k-means function.
```{r k means algorithm}
K_means <- function(dataSet,k){
  #calls the data, and number of clusters desired
  m <- nrow(dataSet)
  n <- ncol(dataSet)
  
  clusters <- rep(0,m) #create a cluster variable with number of rows of 0's.
  center <- dataSet[sample(m,k),] #randomly select k centers
  newcenter <- matrix(0,k,n)
  colnames(newcenter) <- colnames(center)
   #set an empty center same as center
  
  clustertest = TRUE #set clustertest as TRUE
  while (clustertest) {
    centerdist = rdist(dataSet,center) #Euclidean distance measured between data and center
    
    for(i in 1:m){
      clusters[i] = which.min(centerdist[i,])
    }
    #define the cluster
    
    for (j in 1:k){
      newcenter[j,]<- colMeans(dataSet[which(clusters == j),])
    }  
    #define the new center of the data
    
    if (identical(center, newcenter) == TRUE) {clustertest = FALSE}
    else{center <- newcenter}
    #if center is same as new center, clustertest will set as FALSE, which breaks the while loop, else the newcenter will be saved
  }
  
  result = list(clusters = clusters, center = newcenter)
  return (result)
  #the function returns the list of the cluster and the center values
}
```

##Calculate k means
###Wine Data
```{r kmeans}
data<-(wine[-1]) #exclude the "Type" variable

res <- K_means(data,3)
plotcluster(data, res$clusters)
```

As we can see from the plot, the clusters are not very well seperated. There are some overlaps between each clusters.

To quantify how well the algorithm's cluster correspond to the wine types, we will use the rand index.

First, ct.km is used to compare the k means clusters and the original classes of the dataset, which is the types of wines.
```{r randindex}
ct.km <- table(wine$Type, res$clusters)
randIndex(ct.km)
```

The rand index gives the measure of the agreement between two partitions, adjusted for chance. It ranges from -1 (no agreement) to 1 (perfect agreement).

In our case, agreement between the wine type and the cluster solution is not very strong.


###Scaled wine data
```{r scaled data}
data2<-scale(data) #create a new scaled data. 

res2 <- K_means(data2,3)
plotcluster(data2, res2$clusters)
```

The steps are repeated except the data is now scaled. 

Now, the data is clustered very well, not overlaps between clusters.

```{r randindex after scale}
ct.km2 <- table(wine$Type, res2$clusters)
randIndex(ct.km2)
```

After the data is scaled, the rand index become very strong, which is around 0.9. The scaled clusters corresponded to the three wine types very well.

The reason we need scale the data is in order to perform accuracy of distance of each clusters. The k-means minimizes the error function, so normalizing the data improves convergence of such algorithms.

###Iris data
```{r iris}
data(iris) #call iris dataset

data.iris<-iris[-5] #remove the "species" variable

#repeat the steps
res.iris <- K_means(data.iris, 3)
plotcluster(data.iris, res.iris$clusters)
```

From the plot, we can see that 2 clusters are not well seperated.

```{r iris rand index}
ct.km.iris<-table(iris$Species,res.iris$clusters)
randIndex(ct.km.iris)
```

Clearly, the agreement between the iris species and the cluster solution is not very strong.

###Scaled Iris data
```{r scaled iris}
data2.iris <- scale(data.iris)

res2.iris <- K_means(data2.iris, 3)
plotcluster(data2.iris, res2.iris$clusters)
```

After the data is scaled, the clusters still have overlaps.

```{r scaled iris rand index}
ct.km2.iris<-table(iris$Species,res2.iris$clusters)
randIndex(ct.km2.iris)
```

The rand index of scaled iris is close to the original data. The agreement between iris species and the scaled clusters are still not strong.

K means did not work well for classifying the Iris data. Scaling did not improve classifying it.